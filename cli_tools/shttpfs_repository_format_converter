#!/usr/bin/env python
import json, hashlib, sys, shutil
from pprint import pprint
from collections import defaultdict
from datetime import datetime
from copy import deepcopy
import shttpfs.common as sfs
from shttpfs.versioned_storage import versioned_storage

source_directory = ''
manifest_name    = 'manifest_xzf.json'

# The original file store was designed to enable easy browsing of files with a standard
# file browser and it's versioning system was designed to allow manual recovery using
# this method. With experience of using this system for almost two years, it would be
# useful to interact with previous versions from the client similarly to subversion,
# consequently a change of storage format is required. While the original format was
# never designed to provide atomic commits, it does store creation and modification
# times for all items. Using this it is possible to recover the commits using gaps
# in the file timestamps. The following constant defines the maximum time difference
# between two sequential items before following items are considered a new version.

maximum_version_time_delta = 50

# While the previous system was not designed to store delete times of items, this
# information is also available in the timestamps. When a file is revised the new file
# has almost the same timestamp as the previous file, while deleting a file leaves a
# gap in the timestamp if a new file is later inserted with the same name and path.
# This parameter controls the maximum gap between two items with the same path that
# should be treated as modifications. Larger gaps are treated as delete plus insert.
maximum_delete_time_delta  = 71


############################################################################################
# Gets the head and reads all manifest files
############################################################################################
def read_all_manifests():
    #get the head
    head = int(sfs.file_get_contents(sfs.cpjoin(source_directory, 'head')))

    #read the manifests
    prev_head = head - 1
    manifests = []
    i = head
    while True:
        manifest = json.loads(sfs.file_get_contents(sfs.cpjoin(source_directory, 'versions', str(i), manifest_name)))
        manifests.append({f['path'] : f for f in manifest['files']})
        if i == 1: break
        i -= 1

    return list(reversed(manifests))


############################################################################################
# Put all of the files from the manifests in a single array and sort files into their
# creation order as this differs from manifest order
############################################################################################
def flatten_manifests(manifests):
    all_files = []; original_manifest_id = 1 # the index of the original manifest
    for manifest in manifests:
        for cur_name, cur_info in manifest.iteritems():
            all_files.append({'original_manifest_id' : original_manifest_id,
                              'file_info' : cur_info})
        original_manifest_id += 1

    all_files = sorted(all_files, key=lambda x: float(x['file_info']['created']))

    global_file_id = 0
    for fle in all_files:
        fle['global_file_id'] = global_file_id
        global_file_id += 1

    return all_files


############################################################################################
# Detect deleted files by comparing all of the files that have ever existed against
# those that now exist in the head revision. The head always stores the most resent
# state of the system as it exists on the clients.
############################################################################################
def find_deleted_files(all_files, manifests):
    deleted_files = {}
    for fle in all_files: deleted_files[fle['file_info']['path']] = None
    for i in manifests[len(manifests) - 1]: del deleted_files[i]
    return deleted_files


############################################################################################
# Sort deleted items
############################################################################################
def sort_deleted_items_2(all_files, deleted_files):
    def new_item(a, b):
        return {'type' : a, 'items' : b}

    files_by_path = defaultdict(list)
    for fle in all_files:
        files_by_path[fle['file_info']['path']].append(fle)

    processed_deleted_items = []
    for path, items in files_by_path.iteritems():
        previous = None;
        for i in items:
            if previous != None:
                delta = float(i['file_info']['last_mod']) - float(previous['file_info']['created'])
                if delta < 0: raise Exception('Negative time delta between items with the same path!')
                if delta > maximum_delete_time_delta:
                    processed_deleted_items.append(new_item('single', [previous]))

            previous = i

        # always insert delete marker after last item if the group is deleted
        if i['file_info']['path'] in deleted_files:
            processed_deleted_items.append(new_item('single', [items[-1]]))

    return processed_deleted_items


############################################################################################
# When a file is moved back, either because it has been updated or deleted, the file
# creation time is updated to when the file was updated and last modified stores when
# it was originally created. This restores the correct creation time so that files
# appear in the correct order. The modified time is used by the deletion detection
# system to insert a delete marker in the correct place, see next function.
############################################################################################
def fix_creation_times(all_files):
    new_all_files = []
    for fle in all_files:
        i = deepcopy(fle)
        i['file_info']['status'] = 'new' # we don't know the status yet, this may be changed below
        i['file_info']['created'] = i['file_info']['last_mod']
        new_all_files.append(i)
    return new_all_files


############################################################################################
# Inserts delete and move markers
############################################################################################
def apply_deleted_items(new_all_files, processed_deleted_items):
    def new_item(item):
        return {'file_info'           : item, 
                'global_file_id'      : None,
                'original_manifest_id': None}

    for item in processed_deleted_items:
        if item['type'] == 'single':
            file_created_on = item['items'][0]['file_info']['last_mod']
            file_deleted_on = item['items'][0]['file_info']['created']

            if file_deleted_on < file_created_on: raise Exception('File deletion before creation')
            if file_deleted_on == file_created_on: raise Exception('Identical creation time')

            new_all_files.append(new_item({ u'status'     : u'deleted',
                                            u'created' : file_deleted_on,
                                            u'path'     : item['items'][0]['file_info']['path']}))

        else:
            raise Exception('Unknown type')

    # resort so that deleted and moved items appear in the correct place
    new_all_files = sorted(new_all_files, key=lambda x: float(x['file_info']['created']))
    return new_all_files


############################################################################################
# As the previous store was designed to store versions on a per-file basis, to allow
# recovery in case of accidental deletion for instance, we do not have a log of atomic
# commits. However this information is stored in the timestamps. When files are committed 
# this normally happens within a short period of time, thus gaps in the timestamps separate
# commits. This is not exact however as larger files take longer to commit. Groupings need
# to be tuned by adjusting the maximum time delta
############################################################################################
def group_files(all_files, ):
    chunks = defaultdict(list); chunk_id = 0; i = 0
    path_history = {}; path_history_chunk = {}

    while True:
        if i == len(all_files) - 1: time_delta = 0
        else: time_delta = float(all_files[i + 1]['file_info']['created']) - float(all_files[i]['file_info']['created'])

        the_item = all_files[i]

        will_break = False
        if the_item['file_info']['path'] in path_history_chunk:
            will_break = True
        elif time_delta >= maximum_version_time_delta:
            will_break = True

        # Track the paths which have appeared in this chunk, if the same path appears twice we have to start a new chunk
        path_history_chunk[the_item['file_info']['path']] = False

        # Path history is used to correctly label weather items are new or changed relative to previous chunks.
        # Note that this is distinct from the above in that it is ok for a path to be removed when an item
        # is deleted, so that if another file is later added on the same path it is correctly labeled as new.
        if the_item['file_info']['status'] != 'deleted':
            if the_item['file_info']['path'] in path_history:
                the_item['file_info']['status'] = 'changed'
            else:
                the_item['file_info']['status'] = 'new'

            path_history[the_item['file_info']['path']] = False
        else:
            del path_history[the_item['file_info']['path']]


        the_item['time_delta'] = time_delta,
        chunks[chunk_id].append(the_item)

        #===========================
        if will_break == True:
            chunk_id += 1
            path_history_chunk = {}


        if i == len(all_files) - 1: break
        i += 1

    return chunks


############################################################################################
############################################################################################
############################################################################################
############################################################################################
############################################################################################
args = list(sys.argv)[1:]

if args == []:
    print """
Prefix arguments:

-version_time_delta (int)     See note at top of source code.
-delete_time_delta            See note at top of source code.

-list                         List the resulting versions instead of generating new
                              repo format, useful for tuning the above parameters.

-diff                         Compare the contents of the head version with a checkout
                              from the old repository.


[Positional arguments (after above)]

Source directory

Manifest file name
    """

mode = 'convert'
diff_path = ''

while True:
    if args[0] == '-version_time_delta':
        maximum_version_time_delta = int(args[1])
        args = args[2:]

    elif args[0] == '-delete_time_delta':
        maximum_delete_time_delta = int(args[1])
        args = args[2:]

    elif args[0] == '-list':
        mode = 'list'
        args = args[1:]

    elif args[0] == '-diff':
        mode = 'diff'
        diff_path = args[1]
        args = args[2:]

    
    elif args[0] != '-':
        break

    else:
        print 'unknown parameter'
        quit()

source_directory = args[0]
if mode == 'convert':
    output_directory = args[1]
    if len(args) == 3: manifest_name = args[2]


#=====================
manifests = read_all_manifests()
all_files = flatten_manifests(manifests)

# ----------------
deleted_files           = find_deleted_files(all_files, manifests)
processed_deleted_items = sort_deleted_items_2(all_files, deleted_files)

new_all_files = fix_creation_times(all_files)
new_all_files = apply_deleted_items(new_all_files, processed_deleted_items)

grouped_files = group_files(new_all_files)

# It seems to have been possible in the old system for the same file to get changed twice in the same revision
# Handle these by splitting this into two revisions.
#=================================================
fixed_grouped_files = defaultdict(list)
chunk_id = 0
hit = 0
for group_id in sorted(dict(grouped_files).keys()):
    seen_in_chunk = {}
    for item in grouped_files[group_id]:
        if item['file_info']['path'] in seen_in_chunk:
            chunk_id += 1
            hit += 1
            seen_in_chunk = {}

        fixed_grouped_files[chunk_id].append(item)
        seen_in_chunk[item['file_info']['path']] = None
    chunk_id += 1

grouped_files = dict(fixed_grouped_files)


#=====================
if mode == 'list':
    for group_id in sorted(dict(grouped_files).keys()):
        print
        print '------'
        print  '\n' + str(group_id) + '\n'
        for item in grouped_files[group_id]:
            print
            print 'status: '  + item['file_info']['status']
            print 'path: '  + item['file_info']['path']


#=====================
elif mode == 'diff':
    # Rebuild the head state from the diffs
    head_state = {}
    for group_id in sorted(dict(grouped_files).keys()):
        for item in grouped_files[group_id]:
            if item['file_info']['status'] in ['new', 'changed']:
                head_state[item['file_info']['path']] = item

            elif item['file_info']['status'] in ['deleted']:
                del head_state[item['file_info']['path']]
            else: raise SystemExit('error')

    from shttpfs.common import get_file_list
    current_working_copy_state = get_file_list(diff_path)
    for fle in current_working_copy_state:
        if fle['path'] not in head_state:
            print fle['path']


#=====================
elif mode == 'convert':
    new_store = versioned_storage(output_directory)

    # store revisions
    for group_id in sorted(dict(grouped_files).keys()):
        print '=========================='
        print group_id

        # The date and time of the completion of the commit is the latest timestamp in the group
        commit_datetime = max([datetime.utcfromtimestamp(item['file_info']['created']) for item in grouped_files[group_id]])

        # copy the files
        new_store.begin()
        for item in grouped_files[group_id]:
            print item['file_info']['status'] + '    ' +item['file_info']['path']

            if item['file_info']['status'] == 'deleted':
                new_store.fs_delete(item['file_info'])
            else:
                source_base_path = sfs.cpjoin(source_directory, 'versions', str(item['original_manifest_id']))
                shutil.copyfile(sfs.cpjoin(source_base_path, item['file_info']['path']), sfs.cpjoin(output_directory, 'tmp_file'))
                new_store.fs_put_from_file(sfs.cpjoin(output_directory, 'tmp_file'), item['file_info'])

        new_store.commit('', 'Robert', commit_datetime)

